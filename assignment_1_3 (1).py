# -*- coding: utf-8 -*-
"""Copy of assignment_1_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aTz5lsSKYwb9cL__dYi6CgK29t1Yw5e1
"""

from keras.datasets import fashion_mnist
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

(train_X,train_Y),(x_test,y_test)=fashion_mnist.load_data()
train_X = train_X/255
x_test = x_test/255


class Neural_network:
    np.random.seed(10)
    def __init__(self,train_X,train_Y,inp_dim,size_of_hidden_layer,hidden_layers,output_dim,batch_size=30,epochs=10,activation_func="relu"
           ,learning_rate=6e-3 ,decay_rate=0.9,beta=0.9,beta1=0.9,beta2=0.99,optimizer="adam",weight_init="random"):

        self.train_X,self.x_cv,self.train_Y,self.y_cv = train_test_split(train_X, train_Y, test_size=0.10, random_state=100,stratify=train_Y)

        np.random.seed(10)
        self.inp_dim = inp_dim
        self.hidden_layers = hidden_layers
        self.size_of_hidden_layer = size_of_hidden_layer
        self.output_dim = output_dim

        self.batch = batch_size
        self.epochs = epochs
        self.activation_func = activation_func
        self.learning_rate = learning_rate
        self.decay_rate = decay_rate
        self.optimizer = optimizer
        self.weight_init = weight_init
        self.beta = beta
        self.beta1 = beta1
        self.beta2 = beta2


        self.layers = [self.inp_dim] + self.hidden_layers*[self.size_of_hidden_layer] + [self.output_dim]

        layers = self.layers.copy()
        self.weights = []
        self.biases = []
        self.activations = []
        self.activation_grads = []
        self.weights_grads = []
        self.biases_grads = []

        for i in range(len(layers)-1):
            if self.weight_init == 'xavier':
                std = np.sqrt(2/(layers[i]*layers[i+1]))
                self.weights.append(np.random.normal(0,std,(layers[i],layers[i+1])))
                self.biases.append(np.random.normal(0,std,(layers[i+1])))
            else:
                self.weights.append(np.random.normal(0,0.5,(layers[i],layers[i+1])))
                self.biases.append(np.random.normal(0,0.5,(layers[i+1])))
            self.activations.append(np.zeros(layers[i]))
            self.activation_grads.append(np.zeros(layers[i+1]))
            self.weights_grads.append(np.zeros((layers[i],layers[i+1])))
            self.biases_grads.append(np.zeros(layers[i+1]))

        self.activations.append(np.zeros(layers[-1]))

        if optimizer == 'grad_desc':
            self.grad_desc(self.train_X,self.train_Y)
        elif optimizer == 'sgd':
            self.sgd(self.train_X,self.train_Y)
        elif optimizer == 'momentum':
            self.momentum(self.train_X,self.train_Y)
        elif optimizer == 'nesterov':
            self.nesterov(self.train_X,self.train_Y)
        elif optimizer == 'rmsprop':
            self.rmsprop(self.train_X,self.train_Y)
        elif optimizer == 'adam':
            self.adam(self.train_X,self.train_Y)
        elif optimizer == 'nadam':
            self.nadam(self.train_X,self.train_Y)            

    def sigmoid(self,activations):
        res = []
        for z in activations:
            if z>40:
                res.append(1.0)
            elif z<-40:
                res.append(0.0)
            else:
                res.append(1/(1+np.exp(-z)))
        return np.asarray(res)

    def tanh(self,activations):
        res = []
        for z in activations:
            if z>20:
                res.append(1.0)
            elif z<-20:
                res.append(-1.0)
            else:
                res.append((np.exp(z) - np.exp(-z))/(np.exp(z) + np.exp(-z)))
        return np.asarray(res)

    def relu(self,activations):
        res = []
        for i in activations:
            if i<= 0:
                res.append(0)
            else:
                res.append(i)
        return np.asarray(res)

    def softmax(self,activations):
        tot = 0
        for z in activations:
            tot += np.exp(z)
        return np.asarray([np.exp(z)/tot for z in activations])

    def fwd_propagation(self,x,y,weights,biases):
        self.activations[0] = x
        n = len(self.layers)
        for i in range(n-2):
            if self.activation_func == "sigmoid":
                self.activations[i+1] = self.sigmoid(np.matmul(weights[i].T,self.activations[i])+biases[i])
            elif self.activation_func == "tanh":
                self.activations[i+1] = self.tanh(np.matmul(weights[i].T,self.activations[i])+biases[i])
            elif self.activation_func == "relu":
                self.activations[i+1] = self.relu(np.matmul(weights[i].T,self.activations[i])+biases[i])

        self.activations[n-1] = self.softmax(np.matmul(weights[n-2].T,self.activations[n-2])+biases[n-2])        
        return -(np.log2(self.activations[-1][y])) #Return cross entropy loss for single data point.


    def grad_w(self,i):
        return np.matmul(self.activations[i].reshape((-1,1)),self.activation_grads[i].reshape((1,-1)))


    def grad_b(self,i):
        return self.activation_grads[i]


    def bwd_propagation(self,x,y,weights,biases):
        y_onehot = np.zeros(self.output_dim)
        y_onehot[y] = 1
        n = len(self.layers)

        self.activation_grads[-1] =  -1*(y_onehot - self.activations[-1])
        for i in range(n-2,-1,-1):
            self.weights_grads[i] += self.grad_w(i)
            self.biases_grads[i] += self.grad_b(i)
            if i!=0:
                value = np.matmul(weights[i],self.activation_grads[i])
                if self.activation_func == "sigmoid":
                    self.activation_grads[i-1] = value * self.activations[i] * (1-self.activations[i])
                elif self.activation_func == "tanh":
                    self.activation_grads[i-1] = value * (1-np.square(self.activations[i]))
                elif self.activation_func == "relu":
                    res = []
                    for k in self.activations[i]:
                        if k>0: res.append(1.0)
                        else: res.append(0.0)
                    res = np.asarray(res)
                    self.activation_grads[i-1] = value * res



    def grad_desc(self,train_X,train_Y):
        for i in range(self.epochs):
            print('Epoch---',i+1,end=" ")
            loss = 0
            val_loss = 0

            self.weights_grads = [0*i for i in (self.weights_grads)]
            self.biases_grads = [0*i for i in (self.biases_grads)]
            
            index = 1
            for x,y in zip(train_X,train_Y):
                x = x.ravel()
                loss += self.fwd_propagation(x,y,self.weights,self.biases)
                self.bwd_propagation(x,y,self.weights,self.biases)

                if index % self.batch == 0 or index == train_X.shape[0]:
                    for j in range(len(self.weights)):
                        self.weights[j] -= self.learning_rate * self.weights_grads[j]
                        self.biases[j] -= self.learning_rate * self.biases_grads[j]
                    self.weights_grads = [0*i for i in (self.weights_grads)]
                    self.biases_grads = [0*i for i in (self.biases_grads)]
                index += 1 
              
            for x,y in zip(self.x_cv,self.y_cv):
               x=x.ravel()
               val_loss+=self.fwd_propagation(x,y,self.weights,self.biases)

            acc=round(self.calc_accuracy(train_X,train_Y),3)
            val_acc=round(self.calc_accuracy(self.x_cv,self.y_cv),3)
            print('  loss = ',loss/train_X.shape[0],'  accuracy = ',acc,'   validation loss= '
                  ,val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)


    def sgd(self,train_X,train_Y):
        for i in range(self.epochs):
            print('Epoch---',i+1,end=" ")
            loss = 0
            val_loss=0

            index = 1
            for x,y in zip(train_X,train_Y):
                x = x.ravel()
                loss += self.fwd_propagation(x,y,self.weights,self.biases)
                self.bwd_propagation(x,y,self.weights,self.biases)

                if index % self.batch == 0 or index == train_X.shape[0]:
                    for j in range(len(self.weights)):
                        self.weights[j] -= self.learning_rate * self.weights_grads[j]
                        self.biases[j] -= self.learning_rate * self.biases_grads[j]
                    self.weights_grads = [0*i for i in (self.weights_grads)]
                    self.biases_grads = [0*i for i in (self.biases_grads)]

                index += 1
                
            for x,y in zip(self.x_cv,self.y_cv):
               x=x.ravel()
               val_loss+=self.fwd_propagation(x,y,self.weights,self.biases)

            acc=round(self.calc_accuracy(train_X,train_Y),3)
            val_acc=round(self.calc_accuracy(self.x_cv,self.y_cv),3)
            print('  loss = ',loss/train_X.shape[0],'  accuracy = ',acc,'   validation loss= '
                  ,val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)

    def momentum(self,train_X,train_Y):
        prev_grads_w = [0*i for i in (self.weights_grads)]
        prev_grads_b = [0*i for i in (self.biases_grads)]

        for i in range(self.epochs):
            print('Epoch---',i+1,end=" ")
            loss = 0
            val_loss=0

            self.weights_grads = [0*i for i in (self.weights_grads)]
            self.biases_grads = [0*i for i in (self.biases_grads)]

            index = 1
            for x,y in zip(train_X,train_Y):
                x = x.ravel()
                loss += self.fwd_propagation(x,y,self.weights,self.biases)
                self.bwd_propagation(x,y,self.weights,self.biases)
                if index % self.batch == 0 or index == train_X.shape[0]:
                    for j in range(len(self.weights)):
                        v_w = (self.decay_rate * prev_grads_w[j] + self.learning_rate * self.weights_grads[j])
                        v_b = (self.decay_rate * prev_grads_b[j] + self.learning_rate * self.biases_grads[j])
                        self.weights[j] -= v_w
                        self.biases[j] -= v_b
                        prev_grads_w[j] = v_w
                        prev_grads_b[j] = v_b
                    self.weights_grads = [0*i for i in (self.weights_grads)]
                    self.biases_grads = [0*i for i in (self.biases_grads)]

                index +=1
            for x,y in zip(self.x_cv,self.y_cv):
               x=x.ravel()
               val_loss+=self.fwd_propagation(x,y,self.weights,self.biases)

            acc=round(self.calc_accuracy(train_X,train_Y),3)
            val_acc=round(self.calc_accuracy(self.x_cv,self.y_cv),3)
            print('  loss = ',loss/train_X.shape[0],'  accuracy = ',acc,'   validation loss= '
                  ,val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)


    def nesterov(self,train_X,train_Y):
        prev_grads_w = [0*i for i in (self.weights_grads)]
        prev_grads_b = [0*i for i in (self.biases_grads)]

        for i in range(self.epochs):
            print('Epoch---',i+1,end=" ")
            loss = 0
            val_loss=0

            weights = [self.weights[j] -  self.decay_rate * prev_grads_w[j] for j in range(len(self.weights))]
            biases = [self.biases[j] -  self.decay_rate * prev_grads_b[j] for j in range(len(self.biases))]

            self.weights_grads = [0*j for j in (self.weights_grads)]
            self.biases_grads = [0*j for j in (self.biases_grads)]
            index = 1
            for x,y in zip(train_X,train_Y):
                x = x.ravel()
                loss += self.fwd_propagation(x,y,weights,biases)
                self.bwd_propagation(x,y,weights,biases)
                if index % self.batch == 0 or index == train_X.shape[0]:
                    for j in range(len(self.weights)):
                        prev_grads_w[j] = self.decay_rate * prev_grads_w[j] + self.learning_rate*self.weights_grads[j] 
                                           
                        prev_grads_b[j] = self.decay_rate * prev_grads_b[j] + self.learning_rate*self.biases_grads[j] 
                                        
                        self.weights[j] -= prev_grads_w[j]
                        self.biases[j] -= prev_grads_b[j]

                    weights = [self.weights[j] -  self.decay_rate * prev_grads_w[j] for j in range(len(self.weights))]
                    biases = [self.biases[j] -  self.decay_rate * prev_grads_b[j] for j in range(len(self.biases))]

                    self.weights_grads = [0*j for j in (self.weights_grads)]
                    self.biases_grads = [0*j for j in (self.biases_grads)]
                    
                index += 1
            for x,y in zip(self.x_cv,self.y_cv):
               x=x.ravel()
               val_loss+=self.fwd_propagation(x,y,self.weights,self.biases)

            acc=round(self.calc_accuracy(train_X,train_Y),3)
            val_acc=round(self.calc_accuracy(self.x_cv,self.y_cv),3)
            print('  loss = ',loss/train_X.shape[0],'  accuracy = ',acc,'   validation loss= '
                  ,val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)

    def rmsprop(self,train_X,train_Y):
        prev_grads_w = [0*i for i in (self.weights_grads)]
        prev_grads_b = [0*i for i in (self.biases_grads)]
        eps = 1e-2

        for i in range(self.epochs):
            print('Epoch---',i+1,end=" ")
            loss = 0
            val_loss=0

            self.weights_grads = [0*i for i in (self.weights_grads)]
            self.biases_grads = [0*i for i in (self.biases_grads)]

            index = 1
            for x,y in zip(train_X,train_Y):
                x = x.ravel()
                loss += self.fwd_propagation(x,y,self.weights,self.biases)
                self.bwd_propagation(x,y,self.weights,self.biases)
                if index % self.batch == 0 or index == train_X.shape[0]:
                    for j in range(len(self.weights)):
                        v_w = (self.beta * prev_grads_w[j] + (1-self.beta) * np.square(self.weights_grads[j]))
                        v_b = (self.beta * prev_grads_b[j] + (1-self.beta) * np.square(self.biases_grads[j]))
                        self.weights[j] -= self.learning_rate * (self.weights_grads[j] /(np.sqrt(v_w + eps)))
                        self.biases[j] -= self.learning_rate * (self.biases_grads[j] /(np.sqrt(v_b + eps)))
                        prev_grads_w[j] = v_w
                        prev_grads_b[j] = v_b
                    self.weights_grads = [0*i for i in (self.weights_grads)]
                    self.biases_grads = [0*i for i in (self.biases_grads)]

                index +=1
            for x,y in zip(self.x_cv,self.y_cv):
               x=x.ravel()
               val_loss+=self.fwd_propagation(x,y,self.weights,self.biases)

            acc=round(self.calc_accuracy(train_X,train_Y),3)
            val_acc=round(self.calc_accuracy(self.x_cv,self.y_cv),3)
            print('  loss = ',loss/train_X.shape[0],'  accuracy = ',acc,'   validation loss= '
                  ,val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)


    def adam(self,train_X,train_Y):
        m_prev_grads_w = [0*i for i in (self.weights_grads)]
        m_prev_grads_b = [0*i for i in (self.biases_grads)]
        v_prev_grads_w = [0*i for i in (self.weights_grads)]
        v_prev_grads_b = [0*i for i in (self.biases_grads)]

        iter = 1

        for i in range(self.epochs):
            print('Epoch---',i+1,end=" ")
            loss = 0
            val_loss=0
            eps = 1e-2
            self.weights_grads = [0*i for i in (self.weights_grads)]
            self.biases_grads = [0*i for i in (self.biases_grads)]

            index = 1
            for x,y in zip(train_X,train_Y):
                x = x.ravel()
                loss += self.fwd_propagation(x,y,self.weights,self.biases)
                self.bwd_propagation(x,y,self.weights,self.biases)
                if index % self.batch == 0 or index == train_X.shape[0]:
                    for j in range(len(self.weights)):
                        m_w = (self.beta1 * m_prev_grads_w[j] + (1-self.beta1) * self.weights_grads[j])
                        m_b = (self.beta1 * m_prev_grads_b[j] + (1-self.beta1) * self.biases_grads[j])
                        v_w = (self.beta2 * v_prev_grads_w[j] + (1-self.beta2) * np.square(self.weights_grads[j]))
                        v_b = (self.beta2 * v_prev_grads_b[j] + (1-self.beta2) * np.square(self.biases_grads[j]))

                        m_hat_w = (m_w)/(1-(self.beta1)**iter) 
                        m_hat_b = (m_b)/(1-(self.beta1)**iter) 

                        v_hat_w = (v_w)/(1-(self.beta2)**iter) 
                        v_hat_b = (v_b)/(1-(self.beta2)**iter)

                        self.weights[j] -= self.learning_rate * (m_hat_w/(np.sqrt(v_hat_w + eps)))
                        self.biases[j] -= self.learning_rate * (m_hat_b/(np.sqrt(v_hat_b + eps)))

                        m_prev_grads_w[j] = m_w
                        m_prev_grads_b[j] = m_b
                        v_prev_grads_w[j] = v_w
                        v_prev_grads_b[j] = v_b

                    self.weights_grads = [0*i for i in (self.weights_grads)]
                    self.biases_grads = [0*i for i in (self.biases_grads)]
                    iter += 1

                index +=1
                
            for x,y in zip(self.x_cv,self.y_cv):
               x=x.ravel()
               val_loss+=self.fwd_propagation(x,y,self.weights,self.biases)

            acc=round(self.calc_accuracy(train_X,train_Y),3)
            val_acc=round(self.calc_accuracy(self.x_cv,self.y_cv),3)
            print('  loss = ',loss/train_X.shape[0],'  accuracy = ',acc,'   validation loss= '
                  ,val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)
        

    def nadam(self,train_X,train_Y):
        m_prev_grads_w = [0*i for i in (self.weights_grads)]
        m_prev_grads_b = [0*i for i in (self.biases_grads)]
        v_prev_grads_w = [0*i for i in (self.weights_grads)]
        v_prev_grads_b = [0*i for i in (self.biases_grads)]

        iter = 1

        for i in range(self.epochs):
            print('Epoch---',i+1,end=" ")
            loss = 0
            val_loss=0
            eps = 1e-2
            self.weights_grads = [0*i for i in (self.weights_grads)]
            self.biases_grads = [0*i for i in (self.biases_grads)]

            index = 1
            for x,y in zip(train_X,train_Y):
                x = x.ravel()
                loss += self.fwd_propagation(x,y,self.weights,self.biases)
                self.bwd_propagation(x,y,self.weights,self.biases)
                if index % self.batch == 0 or index == train_X.shape[0]:
                    for j in range(len(self.weights)):

                        m_w = (self.beta1 * m_prev_grads_w[j] + (1-self.beta1) * self.weights_grads[j])
                        m_b = (self.beta1 * m_prev_grads_b[j] + (1-self.beta1) * self.biases_grads[j])
                        v_w = (self.beta2 * v_prev_grads_w[j] + (1-self.beta2) * np.square(self.weights_grads[j]))
                        v_b = (self.beta2 * v_prev_grads_b[j] + (1-self.beta2) * np.square(self.biases_grads[j]))

                        m_hat_w = (m_w)/(1-(self.beta1)**iter) 
                        m_hat_b = (m_b)/(1-(self.beta1)**iter) 

                        v_hat_w = (v_w)/(1-(self.beta2)**iter) 
                        v_hat_b = (v_b)/(1-(self.beta2)**iter)

                        m_dash_w = self.beta1 * m_hat_w + (1-self.beta1) * self.weights_grads[j]
                        m_dash_b = self.beta1 * m_hat_b + (1-self.beta1) * self.biases_grads[j]

                        self.weights[j] -= self.learning_rate * (m_dash_w/(np.sqrt(v_hat_w + eps)))
                        self.biases[j] -= self.learning_rate * (m_dash_b/(np.sqrt(v_hat_b + eps)))

                        m_prev_grads_w[j] = m_w
                        m_prev_grads_b[j] = m_b
                        v_prev_grads_w[j] = v_w
                        v_prev_grads_b[j] = v_b

                    self.weights_grads = [0*i for i in (self.weights_grads)]
                    self.biases_grads = [0*i for i in (self.biases_grads)]
                    iter += 1

                index +=1

            for x,y in zip(self.x_cv,self.y_cv):
               x=x.ravel()
               val_loss+=self.fwd_propagation(x,y,self.weights,self.biases)

            acc=round(self.calc_accuracy(train_X,train_Y),3)
            val_acc=round(self.calc_accuracy(self.x_cv,self.y_cv),3)
            print('  loss = ',loss/train_X.shape[0],'  accuracy = ',acc,'   validation loss= '
                  ,val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)


    
    def calc_accuracy(self,X,Y):
        count = 0
        for i in range(len(X)):
            if self.predict(X[i]) == Y[i]:
                count+=1
        return count/len(X)


    def predict(self,x):
        x = x.ravel()
        self.activations[0] = x
        n = len(self.layers)
        for i in range(n-2):
            if self.activation_func == "sigmoid":
                self.activations[i+1] = self.sigmoid(np.matmul(self.weights[i].T,self.activations[i])+self.biases[i])
            elif self.activation_func == "tanh":
                self.activations[i+1] = self.tanh(np.matmul(self.weights[i].T,self.activations[i])+self.biases[i])
            elif self.activation_func == "relu":
                self.activations[i+1] = self.relu(np.matmul(self.weights[i].T,self.activations[i])+self.biases[i])

        self.activations[n-1] = self.softmax(np.matmul(self.weights[n-2].T,self.activations[n-2])+self.biases[n-2])

        return np.argmax(self.activations[-1])
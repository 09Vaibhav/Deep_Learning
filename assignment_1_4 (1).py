# -*- coding: utf-8 -*-
"""Copy of assignment_1_4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Jb3OXubVh2oKrvvXze7JVST60hHuxhWT
"""

from keras.datasets import fashion_mnist
from matplotlib import pyplot as plt


(train_X,train_Y),(test_X,test_Y)=fashion_mnist.load_data()
train_X = train_X/255
test_X = test_X/255

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

class Neural_network:
    np.random.seed(10)
    def __init__(self,train_X,train_Y,inp_dim,size_of_hidden_layer,hidden_layers,output_dim,batch_size=30,epochs=10,activation_func="relu"
           ,learning_rate=6e-3 ,decay_rate=0.9,beta=0.9,beta1=0.9,beta2=0.99,optimizer="adam",weightinit="xavier"):

        self.train_X,self.x_cv,self.train_Y,self.y_cv = train_test_split(train_X, train_Y, test_size=0.10, random_state=100,stratify=train_Y)

        np.random.seed(10)
        self.inp_dim = inp_dim
        self.hidden_layers = hidden_layers
        self.size_of_hidden_layer = size_of_hidden_layer
        self.output_dim = output_dim

        self.batch = batch_size
        self.epochs = epochs
        self.activation_func = activation_func
        self.learning_rate = learning_rate
        self.decay_rate = decay_rate
        self.optimizer = optimizer
        self.weight_init = weight_init
        self.beta = beta
        self.beta1 = beta1
        self.beta2 = beta2

        self.layers = [self.inp_dim] + self.hidden_layers*[self.size_of_hidden_layer] + [self.output_dim]

        layers = self.layers.copy()
        self.weighs = []
        self.biases = []
        self.activations = []
        self.activation_grads = []
        self.weighs_grads = []
        self.biases_grads = []

        for i in range(len(layers)-1):
            if self.weight_init == 'xavier':
                std = np.sqrt(2/(layers[i]*layers[i+1]))
                self.weighs.append(np.random.normal(0,std,(layers[i],layers[i+1])))
                self.biases.append(np.random.normal(0,std,(layers[i+1])))
            else:
                self.weighs.append(np.random.normal(0,0.5,(layers[i],layers[i+1])))
                self.biases.append(np.random.normal(0,0.5,(layers[i+1])))
            self.activations.append(np.zeros(layers[i]))
            self.activation_grads.append(np.zeros(layers[i+1]))
            self.weighs_grads.append(np.zeros((layers[i],layers[i+1])))
            self.biases_grads.append(np.zeros(layers[i+1]))

        self.activations.append(np.zeros(layers[-1]))
        
        if optimizer == 'adam':
            self.adam(self.train_X,self.train_Y)
        elif optimizer == 'sgd':
            self.sgd(self.train_X,self.train_Y)
        elif optimizer == 'momentum':
            self.momentum(self.train_X,self.train_Y)
        elif optimizer == 'nesterov':
            self.nesterov(self.train_X,self.train_Y)
        elif optimizer == 'nadam':
            self.nadam(self.train_X,self.train_Y)
        elif optimizer == 'rmsprop':
            self.rmsprop(self.train_X,self.train_Y)


    def sigmoid(self,activations):
        res = []
        for z in activations:
            if z>40:
                res.append(1.0)
            elif z<-40:
                res.append(0.0)
            else:
                res.append(1/(1+np.exp(-z)))
        return np.asarray(res)

    def tanh(self,activations):
        res = []
        for z in activations:
            if z>20:
                res.append(1.0)
            elif z<-20:
                res.append(-1.0)
            else:
                res.append((np.exp(z) - np.exp(-z))/(np.exp(z) + np.exp(-z)))
        return np.asarray(res)

    def relu(self,activations):
        res = []
        for i in activations:
            if i<= 0:
                res.append(0)
            else:
                res.append(i)
        return np.asarray(res)

    def softmax(self,activations):
        tot = 0
        for z in activations:
            tot += np.exp(z)
        return np.asarray([np.exp(z)/tot for z in activations])



    def fwd_propagation(self,x,y,weighs,biases):
        self.activations[0] = x
        n = len(self.layers)
        for i in range(n-2):
            if self.activation_func == "sigmoid":
                self.activations[i+1] = self.sigmoid(np.matmul(weighs[i].T,self.activations[i])+biases[i])
            elif self.activation_func == "tanh":
                self.activations[i+1] = self.tanh(np.matmul(weighs[i].T,self.activations[i])+biases[i])
            elif self.activation_func == "relu":
                self.activations[i+1] = self.relu(np.matmul(weighs[i].T,self.activations[i])+biases[i])

        self.activations[n-1] = self.softmax(np.matmul(weighs[n-2].T,self.activations[n-2])+biases[n-2])        
        return -(np.log2(self.activations[-1][y])) #Return cross entropy loss for single data point.

    def grad_w(self,i):
        return np.matmul(self.activations[i].reshape((-1,1)),self.activation_grads[i].reshape((1,-1)))


    def grad_b(self,i):
        return self.activation_grads[i]


    def bwd_propagation(self,x,y,weighs,biases):
        y_onehot = np.zeros(self.output_dim)
        y_onehot[y] = 1
        n = len(self.layers)

        self.activation_grads[-1] =  -1*(y_onehot - self.activations[-1])
        for i in range(n-2,-1,-1):
            self.weighs_grads[i] += self.grad_w(i)
            self.biases_grads[i] += self.grad_b(i)
            if i!=0:
                value = np.matmul(weighs[i],self.activation_grads[i])
                if self.activation_func == "sigmoid":
                    self.activation_grads[i-1] = value * self.activations[i] * (1-self.activations[i])
                elif self.activation_func == "tanh":
                    self.activation_grads[i-1] = value * (1-np.square(self.activations[i]))
                elif self.activation_func == "relu":
                    res = []
                    for k in self.activations[i]:
                        if k>0: res.append(1.0)
                        else: res.append(0.0)
                    res = np.asarray(res)
                    self.activation_grads[i-1] = value * res

    def grad_descent(self,train_X,train_Y):
        for i in range(self.epochs):
            print('Epoch---',i+1,end=" ")
            loss = 0

            self.weighs_grads = [0*i for i in (self.weighs_grads)]
            self.biases_grads = [0*i for i in (self.biases_grads)]
            
            index = 1
            for x,y in zip(train_X,train_Y):
                x = x.ravel()
                loss += self.fwd_propagation(x,y,self.weighs,self.biases)
                self.bwd_propagation(x,y,self.weighs,self.biases)

                if index % self.batch == 0 or index == train_X.shape[0]:
                    for j in range(len(self.weighs)):
                        self.weighs[j] -= self.learning_rate * self.weighs_grads[j]
                        self.biases[j] -= self.learning_rate * self.biases_grads[j]
                    self.weighs_grads = [0*i for i in (self.weighs_grads)]
                    self.biases_grads = [0*i for i in (self.biases_grads)]
                index += 1 

            print('  loss = ',loss/train_X.shape[0],'  accuracy = ',round(self.calculate_accuracy(train_X,train_Y),3))


    def sgd(self,train_X,train_Y):
        for i in range(self.epochs):
            print('Epoch---',i+1,end=" ")
            loss = 0
            val_loss=0

            index = 1
            for x,y in zip(train_X,train_Y):
                x = x.ravel()
                loss += self.fwd_propagation(x,y,self.weighs,self.biases)
                self.bwd_propagation(x,y,self.weighs,self.biases)

                if index % self.batch == 0 or index == train_X.shape[0]:
                    for j in range(len(self.weighs)):
                        self.weighs[j] -= self.learning_rate * self.weighs_grads[j]
                        self.biases[j] -= self.learning_rate * self.biases_grads[j]
                    self.weighs_grads = [0*i for i in (self.weighs_grads)]
                    self.biases_grads = [0*i for i in (self.biases_grads)]
                index+=1

            for x,y in zip(self.x_cv,self.y_cv):
               x=x.ravel()
               val_loss+=self.fwd_propagation(x,y,self.weighs,self.biases)

            acc=round(self.calculate_accuracy(train_X,train_Y),3)
            val_acc=round(self.calculate_accuracy(self.x_cv,self.y_cv),3)
            wandb.log({'train_loss':loss/train_X.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuarcy':val_acc})
            print('  loss = ',loss/train_X.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)

    
    def momentum(self,train_X,train_Y):
        prev_grads_w = [0*i for i in (self.weighs_grads)]
        prev_grads_b = [0*i for i in (self.biases_grads)]

        for i in range(self.epochs):
            print('Epoch---',i+1,end=" ")
            loss = 0
            val_loss=0

            self.weighs_grads = [0*i for i in (self.weighs_grads)]
            self.biases_grads = [0*i for i in (self.biases_grads)]

            index = 1
            for x,y in zip(train_X,train_Y):
                x = x.ravel()
                loss += self.fwd_propagation(x,y,self.weighs,self.biases)
                self.bwd_propagation(x,y,self.weighs,self.biases)
                if index % self.batch == 0 or index == train_X.shape[0]:
                    for j in range(len(self.weighs)):
                        v_w = (self.decay_rate * prev_grads_w[j] + self.learning_rate * self.weighs_grads[j])
                        v_b = (self.decay_rate * prev_grads_b[j] + self.learning_rate * self.biases_grads[j])
                        self.weighs[j] -= v_w
                        self.biases[j] -= v_b
                        prev_grads_w[j] = v_w
                        prev_grads_b[j] = v_b
                    self.weighs_grads = [0*i for i in (self.weighs_grads)]
                    self.biases_grads = [0*i for i in (self.biases_grads)]

                index +=1
            for x,y in zip(self.x_cv,self.y_cv):
               x=x.ravel()
               val_loss+=self.fwd_propagation(x,y,self.weighs,self.biases)

            acc=round(self.calculate_accuracy(train_X,train_Y),3)
            val_acc=round(self.calculate_accuracy(self.x_cv,self.y_cv),3)
            wandb.log({'train_loss':loss/train_X.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuarcy':val_acc})
            print('  loss = ',loss/train_X.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)


    def nesterov(self,train_X,train_Y):
        prev_grads_w = [0*i for i in (self.weighs_grads)]
        prev_grads_b = [0*i for i in (self.biases_grads)]

        for i in range(self.epochs):
            print('Epoch---',i+1,end=" ")
            loss = 0
            val_loss=0

            weighs = [self.weighs[j] -  self.decay_rate * prev_grads_w[j] for j in range(len(self.weighs))]
            biases = [self.biases[j] -  self.decay_rate * prev_grads_b[j] for j in range(len(self.biases))]

            self.weighs_grads = [0*j for j in (self.weighs_grads)]
            self.biases_grads = [0*j for j in (self.biases_grads)]
            index = 1
            for x,y in zip(train_X,train_Y):
                x = x.ravel()
                loss += self.fwd_propagation(x,y,weighs,biases)
                self.bwd_propagation(x,y,weighs,biases)
                if index % self.batch == 0 or index == train_X.shape[0]:
                    for j in range(len(self.weighs)):
                        prev_grads_w[j] = self.decay_rate * prev_grads_w[j] + self.learning_rate*self.weighs_grads[j] 
                                           
                        prev_grads_b[j] = self.decay_rate * prev_grads_b[j] + self.learning_rate*self.biases_grads[j] 
                                        
                        self.weighs[j] -= prev_grads_w[j]
                        self.biases[j] -= prev_grads_b[j]

                    weighs = [self.weighs[j] -  self.decay_rate * prev_grads_w[j] for j in range(len(self.weighs))]
                    biases = [self.biases[j] -  self.decay_rate * prev_grads_b[j] for j in range(len(self.biases))]

                    self.weighs_grads = [0*j for j in (self.weighs_grads)]
                    self.biases_grads = [0*j for j in (self.biases_grads)]
                    
                index += 1
            for x,y in zip(self.x_cv,self.y_cv):
               x=x.ravel()
               val_loss+=self.fwd_propagation(x,y,self.weighs,self.biases)

            acc=round(self.calculate_accuracy(train_X,train_Y),3)
            val_acc=round(self.calculate_accuracy(self.x_cv,self.y_cv),3)
            wandb.log({'train_loss':loss/train_X.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuarcy':val_acc})
            print('  loss = ',loss/train_X.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)


    def rmsprop(self,train_X,train_Y):
        prev_grads_w = [0*i for i in (self.weighs_grads)]
        prev_grads_b = [0*i for i in (self.biases_grads)]
        eps = 1e-2

        for i in range(self.epochs):
            print('Epoch---',i+1,end=" ")
            loss = 0
            val_loss=0

            self.weighs_grads = [0*i for i in (self.weighs_grads)]
            self.biases_grads = [0*i for i in (self.biases_grads)]

            index = 1
            for x,y in zip(train_X,train_Y):
                x = x.ravel()
                loss += self.fwd_propagation(x,y,self.weighs,self.biases)
                self.bwd_propagation(x,y,self.weighs,self.biases)
                if index % self.batch == 0 or index == train_X.shape[0]:
                    for j in range(len(self.weighs)):
                        v_w = (self.beta * prev_grads_w[j] + (1-self.beta) * np.square(self.weighs_grads[j]))
                        v_b = (self.beta * prev_grads_b[j] + (1-self.beta) * np.square(self.biases_grads[j]))
                        self.weighs[j] -= self.learning_rate * (self.weighs_grads[j] /(np.sqrt(v_w + eps)))
                        self.biases[j] -= self.learning_rate * (self.biases_grads[j] /(np.sqrt(v_b + eps)))
                        prev_grads_w[j] = v_w
                        prev_grads_b[j] = v_b
                    self.weighs_grads = [0*i for i in (self.weighs_grads)]
                    self.biases_grads = [0*i for i in (self.biases_grads)]

                index +=1
            for x,y in zip(self.x_cv,self.y_cv):
               x=x.ravel()
               val_loss+=self.fwd_propagation(x,y,self.weighs,self.biases)

            acc=round(self.calculate_accuracy(train_X,train_Y),3)
            val_acc=round(self.calculate_accuracy(self.x_cv,self.y_cv),3)
            wandb.log({'train_loss':loss/train_X.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuarcy':val_acc})
            print('  loss = ',loss/train_X.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)


    def adam(self,train_X,train_Y):
        m_prev_grads_w = [0*i for i in (self.weighs_grads)]
        m_prev_grads_b = [0*i for i in (self.biases_grads)]
        v_prev_grads_w = [0*i for i in (self.weighs_grads)]
        v_prev_grads_b = [0*i for i in (self.biases_grads)]

        iter = 1

        for i in range(self.epochs):
            print('Epoch---',i+1,end=" ")
            loss = 0
            val_loss=0
            eps = 1e-2
            self.weighs_grads = [0*i for i in (self.weighs_grads)]
            self.biases_grads = [0*i for i in (self.biases_grads)]

            index = 1
            for x,y in zip(train_X,train_Y):
                x = x.ravel()
                loss += self.fwd_propagation(x,y,self.weighs,self.biases)
                self.bwd_propagation(x,y,self.weighs,self.biases)
                if index % self.batch == 0 or index == train_X.shape[0]:
                    for j in range(len(self.weighs)):
                        m_w = (self.beta1 * m_prev_grads_w[j] + (1-self.beta1) * self.weighs_grads[j])
                        m_b = (self.beta1 * m_prev_grads_b[j] + (1-self.beta1) * self.biases_grads[j])
                        v_w = (self.beta2 * v_prev_grads_w[j] + (1-self.beta2) * np.square(self.weighs_grads[j]))
                        v_b = (self.beta2 * v_prev_grads_b[j] + (1-self.beta2) * np.square(self.biases_grads[j]))

                        m_hat_w = (m_w)/(1-(self.beta1)**iter) 
                        m_hat_b = (m_b)/(1-(self.beta1)**iter) 

                        v_hat_w = (v_w)/(1-(self.beta2)**iter) 
                        v_hat_b = (v_b)/(1-(self.beta2)**iter)

                        self.weighs[j] -= self.learning_rate * (m_hat_w/(np.sqrt(v_hat_w + eps)))
                        self.biases[j] -= self.learning_rate * (m_hat_b/(np.sqrt(v_hat_b + eps)))

                        m_prev_grads_w[j] = m_w
                        m_prev_grads_b[j] = m_b
                        v_prev_grads_w[j] = v_w
                        v_prev_grads_b[j] = v_b

                    self.weighs_grads = [0*i for i in (self.weighs_grads)]
                    self.biases_grads = [0*i for i in (self.biases_grads)]
                    iter += 1

                index +=1
            for x,y in zip(self.x_cv,self.y_cv):
               x=x.ravel()
               val_loss+=self.fwd_propagation(x,y,self.weighs,self.biases)

            acc=round(self.calculate_accuracy(train_X,train_Y),3)
            val_acc=round(self.calculate_accuracy(self.x_cv,self.y_cv),3)
            wandb.log({'train_loss':loss/train_X.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuarcy':val_acc})
            print('  loss = ',loss/train_X.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)

    def nadam(self,train_X,train_Y):
        m_prev_grads_w = [0*i for i in (self.weighs_grads)]
        m_prev_grads_b = [0*i for i in (self.biases_grads)]
        v_prev_grads_w = [0*i for i in (self.weighs_grads)]
        v_prev_grads_b = [0*i for i in (self.biases_grads)]

        iter = 1

        for i in range(self.epochs):
            print('Epoch---',i+1,end=" ")
            loss = 0
            val_loss=0
            eps = 1e-2
            self.weighs_grads = [0*i for i in (self.weighs_grads)]
            self.biases_grads = [0*i for i in (self.biases_grads)]

            index = 1
            for x,y in zip(train_X,train_Y):
                x = x.ravel()
                loss += self.fwd_propagation(x,y,self.weighs,self.biases)
                self.bwd_propagation(x,y,self.weighs,self.biases)
                if index % self.batch == 0 or index == train_X.shape[0]:
                    for j in range(len(self.weighs)):

                        m_w = (self.beta1 * m_prev_grads_w[j] + (1-self.beta1) * self.weighs_grads[j])
                        m_b = (self.beta1 * m_prev_grads_b[j] + (1-self.beta1) * self.biases_grads[j])
                        v_w = (self.beta2 * v_prev_grads_w[j] + (1-self.beta2) * np.square(self.weighs_grads[j]))
                        v_b = (self.beta2 * v_prev_grads_b[j] + (1-self.beta2) * np.square(self.biases_grads[j]))

                        m_hat_w = (m_w)/(1-(self.beta1)**iter) 
                        m_hat_b = (m_b)/(1-(self.beta1)**iter) 

                        v_hat_w = (v_w)/(1-(self.beta2)**iter) 
                        v_hat_b = (v_b)/(1-(self.beta2)**iter)

                        m_dash_w = self.beta1 * m_hat_w + (1-self.beta1) * self.weighs_grads[j]
                        m_dash_b = self.beta1 * m_hat_b + (1-self.beta1) * self.biases_grads[j]

                        self.weighs[j] -= self.learning_rate * (m_dash_w/(np.sqrt(v_hat_w + eps)))
                        self.biases[j] -= self.learning_rate * (m_dash_b/(np.sqrt(v_hat_b + eps)))

                        m_prev_grads_w[j] = m_w
                        m_prev_grads_b[j] = m_b
                        v_prev_grads_w[j] = v_w
                        v_prev_grads_b[j] = v_b

                    self.weighs_grads = [0*i for i in (self.weighs_grads)]
                    self.biases_grads = [0*i for i in (self.biases_grads)]
                    iter += 1

                index +=1

            for x,y in zip(self.x_cv,self.y_cv):
               x=x.ravel()
               val_loss+=self.fwd_propagation(x,y,self.weighs,self.biases)

            acc=round(self.calculate_accuracy(train_X,train_Y),3)
            val_acc=round(self.calculate_accuracy(self.x_cv,self.y_cv),3)
            wandb.log({'train_loss':loss/train_X.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuarcy':val_acc})
            print('  loss = ',loss/train_X.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)


    def calculate_accuracy(self,X,Y):
        count = 0
        for i in range(len(X)):
            if self.predict(X[i]) == Y[i]:
                count+=1
        return count/len(X)


    def predict(self,x):
        x = x.ravel()
        self.activations[0] = x
        n = len(self.layers)
        for i in range(n-2):
            if self.activation_func == "sigmoid":
                self.activations[i+1] = self.sigmoid(np.matmul(self.weighs[i].T,self.activations[i])+self.biases[i])
            elif self.activation_func == "tanh":
                self.activations[i+1] = self.tanh(np.matmul(self.weighs[i].T,self.activations[i])+self.biases[i])
            elif self.activation_func == "relu":
                self.activations[i+1] = self.relu(np.matmul(self.weighs[i].T,self.activations[i])+self.biases[i])

        self.activations[n-1] = self.softmax(np.matmul(self.weighs[n-2].T,self.activations[n-2])+self.biases[n-2])

        return np.argmax(self.activations[-1])

sweep_config={
    'method': 'random',
    'metric': {
        'name': 'accuracy',
        'goal': 'maximize'
    },
    'parameters':{
        'epochs':{
            'values':[3,5,7]
        },
        'batch_size':{
            'values':[32,64,128]
        },
        'hidden_layers':{
            'values':[1,2,3]
        },
        'size_of_hidden_layer':{
            'values':[16,32,64]
        },
        'learning_rate':{
            'values':[5e-3,2e-3,6e-3,5e-4]
        },
        'weight_decay':{
            'values':[0.1,0.0,0.8,0.9]
        },
        'optimizer':{
            'values':['sgd','momentum','nesterov','adam','rmsprop','nadam']
        },
        'activation':{
            'values':['sigmoid','tanh','relu']
        },
        'weight_init':{
            'values':['random','xavier']
        }
    }
}

!pip install --upgrade wandb
import wandb
!wandb login 7c2b51b201fca423d3798d6c77c45bc79501464a

sweep_id = wandb.sweep(sweep_config,project="question_4", entity="kumar-vaibhav")

def train():
    config_defaults={
      'epochs':5,
      'batch_size':16,
      'learning_rate':1e-3,
      'activation':'relu',
      'optimizer':'adam',
      'size_of_hidden_layer':32,
      'hidden_layers':3,
      'weight_init':'xavier' }
    
    
    
    wandb.init(config=config_defaults)
    config=wandb.config

    Neural_network(train_X,train_Y,len(train_X[0].ravel()),config.size_of_hidden_layer,config.hidden_layers,max(train_Y)+1,
                       config.batch_size,config.epochs,config.activation,
                 config.learning_rate,config.weight_decay,0.9,0.9,0.99
                        ,config.optimizer,config.weight_init)

wandb.agent(sweep_id,train)